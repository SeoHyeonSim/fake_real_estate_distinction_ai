# -*- coding: utf-8 -*-
"""NewTrainDataset-XGBoost-trial-Optuna-Smote.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yob8CEJkQK_JF4oRDhxnuMl6yxRXB_u7

## 사전 준비
"""

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

"""# New Train Data"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier

# importing packages

# 기본 패키지

import pandas as pd
import numpy as np

# 시각화 패키지

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns

# 데이터 분석 패키지

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression

train = pd.read_csv('/content/drive/MyDrive/KUBIC_DL_project/data/new_train.csv')

train.head()

train.info()

"""# Test Dataset"""

test = pd.read_csv('/content/drive/MyDrive/KUBIC_DL_project/data/test.csv')

test.head()

# test 데이터에 NA개수 컬럼 추가 (각 행의 결측치 수)
test['NA개수'] = test.isnull().sum(axis=1)

test.head()

test.info()

"""# 데이터 전처리"""

for df in [train, test]:
    df['게재일'] = pd.to_datetime(df['게재일'])
    df['게재년'] = df['게재일'].dt.year
    df['게재월'] = df['게재일'].dt.month
    df['게재일자'] = df['게재일'].dt.day
    df.drop('게재일', axis=1, inplace=True)

ID_col = 'ID'
target_col = '허위매물여부'

# 학습에 사용할 feature: ID와 target_col을 제외한 모든 컬럼
features = [col for col in train.columns if col not in [ID_col, target_col]]
target = '허위매물여부'

# 범주형: 매물확인방식, 방향, 주차가능여부, 중개사무소, 제공플랫폼
categorical_features = ['매물확인방식', '방향', '주차가능여부', '중개사무소', '제공플랫폼']

numeric_features = [col for col in features if col not in categorical_features]

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

# 수치형 변수 전처리: 결측치 중앙값 대체 + SimpleImputer
# numeric_pipeline = Pipeline(steps=[
#     ('imputer', SimpleImputer(strategy='median')),
#     ('scaler', StandardScaler())
# ])

# 수치형 변수 전처리: KNNImputer + StandardScaler
numeric_pipeline = Pipeline(steps=[
    ('imputer', KNNImputer(n_neighbors=5)),
    ('scaler', StandardScaler())
])

# 범주형 변수 전처리: 결측치는 최빈값으로 대체한 후 One-Hot Encoding
categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_pipeline, numeric_features),
        ('cat', categorical_pipeline, categorical_features)
    ]
)

# 전처리 후, train 데이터와 test 데이터를 변환
X_train = train[features]
y_train = train[target]

X_train_preprocessed = preprocessor.fit_transform(X_train)
print("전처리 train 데이터 shape:", X_train_preprocessed.shape)

X_test = test.drop('ID', axis=1)  # test 데이터에는 target이 없으므로 'ID'만 제외
X_test_preprocessed = preprocessor.transform(X_test)
print("전처리된 test 데이터 shape:", X_test_preprocessed.shape)

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from xgboost import XGBClassifier

X_train_split, X_val, y_train_split, y_val = train_test_split(
    X_train_preprocessed, y_train, test_size=0.2, random_state=42, stratify=y_train
)

"""* Optuna 최적화 적용"""

!pip install optuna

import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import StratifiedKFold, cross_val_score
from xgboost import XGBClassifier
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
import numpy as np

def objective(trial):
    # 하이퍼파라미터 제안
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 30),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),
        'use_label_encoder': False,
        'eval_metric': 'logloss',
        'random_state': 42,
        'n_jobs': -1
    }

    # XGBoost 모델 생성
    model = XGBClassifier(**params)

    # SMOTE와 XGBoost를 포함한 파이프라인 구성
    pipeline = ImbPipeline(steps=[
        ('smote', SMOTE(random_state=42)),
        ('classifier', model)
    ])

    # Stratified 5-Fold 교차 검증 수행
    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(pipeline, X_train_preprocessed, y_train, cv=kf, scoring='accuracy', n_jobs=-1)

    return np.mean(cv_scores)

# Optuna 스터디 생성 및 최적화 진행
study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
study.optimize(objective, n_trials=100, show_progress_bar=True)


print("Best Params:", study.best_params)
print("Best CV Accuracy:", study.best_value)

best_params = study.best_params
best_params.update({'use_label_encoder': False, 'eval_metric': 'logloss', 'random_state': 42, 'n_jobs': -1})
final_model = XGBClassifier(**best_params)

# 최종 파이프라인: SMOTE와 최적의 XGBoost 모델을 포함
final_pipeline = ImbPipeline(steps=[
    ('smote', SMOTE(random_state=42)),
    ('classifier', final_model)
])

final_pipeline.fit(X_train_preprocessed, y_train)

y_val_pred = final_pipeline.predict(X_val)
print(classification_report(y_val, y_val_pred))
print("Accuracy:", accuracy_score(y_val, y_val_pred))

final_pipeline.fit(X_train_preprocessed, y_train) # 전체 train 데이터로 모델 재학습
X_test = test[features]  # test 데이터의 feature 컬럼은 train과 동일해야 함
test_preds = final_pipeline.predict(X_test_preprocessed)

submission = pd.DataFrame({
    'ID': test[ID_col],
    '허위매물여부': test_preds
})

submission.to_csv('/content/drive/MyDrive/KUBIC_DL_project/submissions/submission_xgb_new_8.csv', index=False)

"""최종 성능 : 0.8511029412


"""